{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree based methods for prediction\n",
    "\n",
    "This tutorial is meant to be an introduction to tree based methods for prediction. We start with the most basic model, a decision tree, and work our way up to the more recent work on gradient boosting. We will be alternating between sample datasets available with sci-kit learn and a medical dataset made available in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import datasets\n",
    "import pydotplus\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# used to display trees\n",
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_model_pred_2d(mdl, X, y, feat):\n",
    "    # look at the regions in a 2d plot\n",
    "    # based on scikit-learn tutorial plot_iris.html\n",
    "\n",
    "    # get minimum and maximum values\n",
    "    x0_min = X[:, 0].min()\n",
    "    x0_max = X[:, 0].max()\n",
    "    x1_min = X[:, 1].min()\n",
    "    x1_max = X[:, 1].max()\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(x0_min, x0_max, 100),\n",
    "                         np.linspace(x1_min, x1_max, 100))\n",
    "\n",
    "    Z = mdl.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # plot the contour - colouring different regions\n",
    "    cs = plt.contourf(xx, yy, Z, cmap='Blues')\n",
    "\n",
    "    # plot the individual data points - colouring by the *true* outcome\n",
    "    color = y.ravel()\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=color, marker='o', s=40, cmap='Blues')\n",
    "\n",
    "    plt.xlabel(feat[0])\n",
    "    plt.ylabel(feat[1])\n",
    "    plt.axis(\"tight\")\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "The dataset we'll use is the UCI Breast Cancer classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# real example\n",
    "df = datasets.load_iris()\n",
    "\n",
    "# if you want a description of the dataset, uncomment the below line\n",
    "print(df['DESCR'])\n",
    "\n",
    "idx = [0,2]\n",
    "X = df['data'][50:,idx]\n",
    "y = df['target'][50:]\n",
    "\n",
    "feat = [df['feature_names'][x] for x in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "\n",
    "There are a few flavours - but we will be using Classification and Regression Trees (CART) - well described in a book of the same name [1]. This section will go over an two example datasets: one for classification and one for regression.\n",
    "\n",
    "* build a single tree for each\n",
    "* examine how the tree partitions the space\n",
    "* explain how this works in terms of the cost function split on at each node\n",
    "\n",
    "[1] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdl = tree.DecisionTreeClassifier(criterion='entropy', splitter='best')\n",
    "mdl = mdl.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# examine the tree\n",
    "tree.export_graphviz(mdl, out_file='tree.dot',\n",
    "                         feature_names=feat, \n",
    "                         filled=True, rounded=True)  \n",
    "graph = pydotplus.graphviz.graph_from_dot_file(\"tree.dot\") \n",
    "Image(graph.create_png())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look at the regions in a 2d plot\n",
    "# based on scikit-learn tutorial plot_iris.html\n",
    "plt.figure(figsize=[16,9])\n",
    "plot_model_pred_2d(mdl, X, y, feat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* above model is complicated\n",
    "* likely \"overfit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth=1)\n",
    "mdl = ensemble.AdaBoostClassifier(base_estimator=clf,n_estimators=5)\n",
    "mdl = mdl.fit(X,y)\n",
    "\n",
    "\n",
    "# plot the final prediction\n",
    "plt.figure(figsize=[16,9])\n",
    "plot_model_pred_2d(mdl, X, y, feat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# break it down into 5 subfigures\n",
    "\n",
    "print('Each estimator, one at a time...')\n",
    "fig = plt.figure(figsize=[16,9])\n",
    "for i, estimator in enumerate(mdl.estimators_):\n",
    "    ax = fig.add_subplot(2,3,i+1)\n",
    "    plot_model_pred_2d(estimator, X, feat)\n",
    "    plt.title('Estimator {}'.format(i+1),fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot the final prediction\n",
    "plt.figure(figsize=[16,9])\n",
    "plot_model_pred_2d(mdl, X, y, feat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# break it down into 5 subfigures\n",
    "clf = tree.DecisionTreeClassifier(max_depth=3)\n",
    "mdl = ensemble.AdaBoostClassifier(base_estimator=clf,n_estimators=5)\n",
    "mdl = mdl.fit(X,y)\n",
    "\n",
    "print('Each estimator, one at a time...')\n",
    "fig = plt.figure(figsize=[16,9])\n",
    "for i, estimator in enumerate(mdl.estimators_):\n",
    "    ax = fig.add_subplot(2,3,i+1)\n",
    "    plot_model_pred_2d(estimator, X, y, feat)\n",
    "    plt.title('Estimator {}'.format(i+1),fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot the final prediction\n",
    "plt.figure(figsize=[16,9])\n",
    "plot_model_pred_2d(mdl, X, y, feat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "Bagging is a form of *ensemble learning*: we aim to build a single good model by combining many models together. For example, we could train 10 decision trees, and then combine the predictions of those 10 decision trees (say by majority vote or averaging) in order to get a better answer.\n",
    "\n",
    "It's not quite that simple though. In the above CART, recall that the training method involved (i) determining the optimal split point across all features, (ii) splitting the data into two groups, and (iii) repeating this process until completion. If we gave the decision tree the same dataset, we would always get the same tree built - which means there is absolutely no point in repeatedly building trees. Ideally, we would be able to acquire a new dataset each time we decide to build a new tree. Practically that's usually not possible, but happily we can take advantage of a magic trick from statistics: the bootstrap.\n",
    "\n",
    "The bootstrap is a very popular method, and loosely speaking it allows one to repeatedly generate \"new\" datasets from the current dataset. So our new methodology is:\n",
    "\n",
    "1. Generate a bootstrap sample of the dataset\n",
    "2. Build a decision tree\n",
    "3. Repeat 1-2 until we're happy with the overall model\n",
    "\n",
    "And that's it! Let's try it with the housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth=1)\n",
    "mdl = ensemble.BaggingClassifier(base_estimator=clf, n_estimators=5)\n",
    "mdl = mdl.fit(X,y)\n",
    "\n",
    "\n",
    "\n",
    "print('Each estimator, one at a time...')\n",
    "fig = plt.figure(figsize=[16,9])\n",
    "for i, estimator in enumerate(mdl.estimators_):\n",
    "    ax = fig.add_subplot(2,3,i+1)\n",
    "    plot_model_pred_2d(estimator, X, y, feat)\n",
    "    plt.title('Estimator {}'.format(i+1),fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot the final prediction\n",
    "plt.figure(figsize=[16,9])\n",
    "plot_model_pred_2d(mdl, X, y, feat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth=1)\n",
    "mdl = ensemble.BaggingClassifier(base_estimator=clf, n_estimators=10)\n",
    "mdl = mdl.fit(X,y)\n",
    "\n",
    "\n",
    "\n",
    "print('Each estimator, one at a time...')\n",
    "fig = plt.figure(figsize=[16,9])\n",
    "for i, estimator in enumerate(mdl.estimators_):\n",
    "    ax = fig.add_subplot(3,4,i+1)\n",
    "    plot_model_pred_2d(estimator, X, y, feat)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot the final prediction\n",
    "plt.figure(figsize=[16,9])\n",
    "plot_model_pred_2d(mdl, X, y, feat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth=None)\n",
    "mdl = ensemble.BaggingClassifier(base_estimator=clf, n_estimators=10)\n",
    "mdl = mdl.fit(X,y)\n",
    "\n",
    "\n",
    "\n",
    "print('Each estimator, one at a time...')\n",
    "fig = plt.figure(figsize=[16,9])\n",
    "for i, estimator in enumerate(mdl.estimators_):\n",
    "    ax = fig.add_subplot(3,4,i+1)\n",
    "    plot_model_pred_2d(estimator, X, y, feat)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot the final prediction\n",
    "plt.figure(figsize=[16,9])\n",
    "plot_model_pred_2d(mdl, X, y, feat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we used the bootstrap to randomly resample our data to generate \"new\" datasets to build trees from. The Random Forest takes this one step further: instead of just resampling our data, we also select only a fraction of the features to include. It turns out that this extra randomization tends to improve the performance of our models.\n",
    "\n",
    "Let's try and train the model now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdl = ensemble.RandomForestClassifier(n_estimators=10)\n",
    "mdl = mdl.fit(X,y)\n",
    "\n",
    "plt.figure(figsize=[16,9])\n",
    "plot_model_pred_2d(mdl, X, y, feat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "From scikit manual\n",
    "\n",
    "The advantages of GBRT are:\n",
    "* Natural handling of data of mixed type (= heterogeneous features)\n",
    "* Predictive power\n",
    "* Robustness to outliers in output space (via robust loss functions)\n",
    "\n",
    "The disadvantages of GBRT are:\n",
    "* Scalability, due to the sequential nature of boosting it can hardly be parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdl = ensemble.GradientBoostingClassifier(n_estimators=10)\n",
    "mdl = mdl.fit(X, y)\n",
    "\n",
    "\n",
    "plt.figure(figsize=[16,9])\n",
    "plot_model_pred_2d(mdl, X, y, feat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running through a slightly harder dataset\n",
    "\n",
    "Breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# real example\n",
    "df = datasets.load_breast_cancer()\n",
    "\n",
    "# if you want a description of the dataset, uncomment the below line\n",
    "print(df['DESCR'])\n",
    "\n",
    "# pick index of the features to use (only pick 2)\n",
    "#    :Attribute Information (in order):\n",
    "#        0 - CRIM     per capita crime rate by town\n",
    "#        1 - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "#        2 - INDUS    proportion of non-retail business acres per town\n",
    "#        3 - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "#        4 - NOX      nitric oxides concentration (parts per 10 million)\n",
    "#        5 - RM       average number of rooms per dwelling\n",
    "#        6 - AGE      proportion of owner-occupied units built prior to 1940\n",
    "#        7 - DIS      weighted distances to five Boston employment centres\n",
    "#        8 - RAD      index of accessibility to radial highways\n",
    "#        9 - TAX      full-value property-tax rate per $10,000\n",
    "#       10 - PTRATIO  pupil-teacher ratio by town\n",
    "#       11 - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "#       12 - LSTAT    % lower status of the population\n",
    "#       13 - MEDV     Median value of owner-occupied homes in $1000's\n",
    "\n",
    "\n",
    "idx = [1,29]\n",
    "X = df['data'][:,idx]\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.30, random_state=42)\n",
    "\n",
    "feat = [x for x in df['feature_names'][idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = dict()\n",
    "#clf['Decision Tree'] = tree.DecisionTreeClassifier(criterion='entropy', splitter='best').fit(X,y)\n",
    "clf['Gradient Boosting'] = GradientBoostingClassifier(n_estimators=10).fit(X_train, y_train)\n",
    "clf['Random Forest'] = ensemble.RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n",
    "clf['Bagging'] =  ensemble.BaggingClassifier(n_estimators=10).fit(X_train, y_train)\n",
    "clf['AdaBoost'] =  ensemble.AdaBoostClassifier(n_estimators=10).fit(X_train, y_train)\n",
    "\n",
    "fig = plt.figure(figsize=[16,9])\n",
    "for i, curr_mdl in enumerate(clf):\n",
    "    score = clf[curr_mdl].score(X_test, y_test)\n",
    "    print('{}\\t{}'.format(score, curr_mdl))\n",
    "    \n",
    "    ax = fig.add_subplot(2,2,i+1)\n",
    "    plot_model_pred_2d(clf[curr_mdl], X_test, y_test, feat)\n",
    "    \n",
    "    plt.title(curr_mdl)\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# even better, use cross-validation\n",
    "for curr_mdl in clf:\n",
    "    scores = model_selection.cross_val_score(clf[curr_mdl], X, y, cv=5, scoring='accuracy')\n",
    "    auc = model_selection.cross_val_score(clf[curr_mdl], X, y, cv=5, scoring='roc_auc')\n",
    "    print('{:0.3f}\\t{:0.3f}\\t{}'.format(scores.mean(), auc.mean(), curr_mdl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "We'll now practice using these models on a dataset acquired from patients admitted to intensive care units at the Beth Israel Deaconness Medical Center in Boston, MA. All patients in the cohort stayed for at least 48 hours, and the goal of the prediction task is to predict in-hospital mortality. If you're interested, you can read more about the dataset here: http://physionet.org/challenge/2012/\n",
    "\n",
    "The following command will download a \"preprocessed\" version of the dataset to the current directory. The data is originally provided as hourly observations for a number of variables, and the preprocessing step involved extracting summary statistics across all these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first five features to get an idea of what this dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* build a single tree for each\n",
    "* examine how the tree partitions the space\n",
    "* explain how this works in terms of the cost function split on at each node\n",
    "\n",
    "End of section exercise: build a decision tree using PhysionetChallenge2012_train.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
